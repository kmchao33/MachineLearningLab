{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run preprocessing\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files and construct term frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  BMW-AnnualReport-2017.json\n",
      "running time:  12.872901439666748\n",
      "processing:  BVB-AnnualReport-2015.json\n",
      "running time:  7.554543733596802\n",
      "processing:  BVB-AnnualReport-2016.json\n",
      "running time:  8.048486471176147\n",
      "processing:  BVB-AnnualReport-2017.json\n",
      "running time:  9.21908950805664\n",
      "processing:  CarlZeissMeditec-AnnualReport-2017.json\n",
      "running time:  8.141857147216797\n",
      "processing:  CarlZeissMeditec-AnnualReport-2015.json\n",
      "running time:  7.0159618854522705\n",
      "processing:  CarlZeissMeditec-AnnualReport-2016.json\n",
      "running time:  7.3509156703948975\n",
      "processing:  BMW-AnnualReport-2016.json\n",
      "running time:  12.51590609550476\n",
      "processing:  BMW-AnnualReport-2015.json\n",
      "running time:  14.098376989364624\n",
      "CPU times: user 11min 6s, sys: 30.4 s, total: 11min 36s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# obtain term-frequency matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import string\n",
    "import time\n",
    "word2index = {}\n",
    "document2index = {}\n",
    "index2document = {}\n",
    "document_word_vectors = {}\n",
    "w_cnt = 0\n",
    "d_cnt = 0\n",
    "nlp = spacy.load('de')\n",
    "for root, dirs, files in os.walk('sample'):\n",
    "    for f in files:\n",
    "        document_word_vectors[f] = []\n",
    "        document2index[f] = d_cnt\n",
    "        index2document[d_cnt] = f\n",
    "        d_cnt+=1\n",
    "        with open(root+'/'+f) as fs:\n",
    "            print('processing: ', f)\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                for line in fs:\n",
    "                    #loads json file, preprocess the content\n",
    "                    obj = json.loads(line)\n",
    "                    textType = obj['type']\n",
    "                    if textType == 'paragraph':\n",
    "                        #remove digits\n",
    "                        dig_translator = str.maketrans('', '', '0123456789')\n",
    "                        text = obj['content'].lower().translate(dig_translator)\n",
    "                        #remove punctuation\n",
    "                        str_translator = str.maketrans('', '', string.punctuation)\n",
    "                        text = text.translate(str_translator)\n",
    "                        documents = nlp(text)\n",
    "                        for w in documents:\n",
    "                            if w.is_stop == False and w.is_punct == False and w.is_digit == False and w.is_space == False:\n",
    "                                if w.lemma_ not in word2index.keys():\n",
    "                                    word2index[w.lemma_] = w_cnt\n",
    "                                    w_cnt+=1\n",
    "                                document_word_vectors[f].append(word2index[w.lemma_])\n",
    "#                         words = textPreprocess(obj['content'])\n",
    "#                         for w in words:\n",
    "#                             if w not in word2index:\n",
    "#                                 #reserve word_index dict, for referening later on\n",
    "#                                 word2index[w] = w_cnt\n",
    "#                                 w_cnt+=1\n",
    "#                             document_word_vectors[f].append(word2index[w])\n",
    "            except:\n",
    "                print (f)\n",
    "            end_time = time.time()\n",
    "            print('running time: ', end_time-start_time)\n",
    "\n",
    "#create word_frequency matrix                        \n",
    "w_f_matrix = np.zeros((len(word2index),len(document2index)))\n",
    "for doc in document_word_vectors:\n",
    "    i = document2index[doc]\n",
    "    for j in document_word_vectors[doc]:\n",
    "        w_f_matrix[j,i]+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain term-frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain normalized term-frequency matrix\n",
    "t_f = np.copy(w_f_matrix)\n",
    "sum_f = np.zeros(len(document2index))\n",
    "for i in range(len(document2index)):\n",
    "    sum_f[i] = np.sum(t_f[:,i])\n",
    "t_f = np.divide(t_f,sum_f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining tf-idf matrix\n",
    "inv_doc_freq = np.count_nonzero(t_f,axis=1)\n",
    "def normalize(a,x):\n",
    "    return np.log(x/a)\n",
    "norm = np.vectorize(normalize)\n",
    "inv_doc_freq = norm(inv_doc_freq,len(document2index))\n",
    "tf_idf = np.multiply(t_f,inv_doc_freq.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20113, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.93019491e-04, 1.51762151e-04, 8.25878619e-05, ...,\n",
       "        0.00000000e+00, 3.93019491e-04, 4.05164129e-04],\n",
       "       [1.70923424e-04, 9.42872417e-05, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.70923424e-04, 1.84595824e-04],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.27312447e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.27312447e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.27312447e-05]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most frequent words in each documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "\n",
    "def find_top_k_words(tf_idf, k):\n",
    "    for i in range(tf_idf.shape[1]):\n",
    "        freq_words = []\n",
    "        #get indices of k-maximum values in numpy column\n",
    "        index = np.argpartition(tf_idf[:, i], -k)[-k:]\n",
    "        index = index[np.argsort(tf_idf[:, i][index])]\n",
    "        print('Most frequent words in: ', index2document[i])\n",
    "        for ind in index:\n",
    "            #find frequent words with coresponding index\n",
    "            freq_words.append(list(word2index.keys())[list(word2index.values()).index(ind)])\n",
    "        print(freq_words)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words in:  BMW-AnnualReport-2017.json\n",
      "['mio', 'motorräder', 'vorzugsaktien', 'fahrzeug', '€', 'group', 'fahrzeuge', 'mini', 'automobile', 'bmw']\n",
      "Most frequent words in:  BVB-AnnualReport-2015.json\n",
      "['haftend', 'mannschaft', 'league', 'iduna', 'uefa', 'saison', 'eur', 'borussia', 'teur', 'dortmund']\n",
      "Most frequent words in:  BVB-AnnualReport-2016.json\n",
      "['haftend', 'iduna', 'saison', 'bvb', 'fc', 'league', 'uefa', 'teur', 'borussia', 'dortmund']\n",
      "Most frequent words in:  BVB-AnnualReport-2017.json\n",
      "['sportlich', 'haftend', 'bvb', 'uefa', 'iduna', 'saison', 'eur', 'borussia', 'teur', 'dortmund']\n",
      "Most frequent words in:  CarlZeissMeditec-AnnualReport-2017.json\n",
      "['thompson', 'devices', '€', 'veracity', 'ophthalmic', 'patienten', 'tsd', 'meditec', 'carl', 'zeiss']\n",
      "Most frequent words in:  CarlZeissMeditec-AnnualReport-2015.json\n",
      "['geschäftseinheit', '€', 'diagnose', 'jena', 'oraya', 'vj', 'tsd', 'meditec', 'carl', 'zeiss']\n",
      "Most frequent words in:  CarlZeissMeditec-AnnualReport-2016.json\n",
      "['ophthalmic', 'devices', 'sbu', 'patienten', 'mio€', 'vj', 'tsd', 'meditec', 'carl', 'zeiss']\n",
      "Most frequent words in:  BMW-AnnualReport-2016.json\n",
      "['mio', 'motorräder', 'vorzugsaktien', 'fahrzeug', '€', 'group', 'fahrzeuge', 'mini', 'automobile', 'bmw']\n",
      "Most frequent words in:  BMW-AnnualReport-2015.json\n",
      "['fahrzeug', 'modelle', 'motorräder', 'fahrzeuge', 'mini', 'group', 'mio', 'automobile', '€', 'bmw']\n"
     ]
    }
   ],
   "source": [
    "find_top_k_words(tf_idf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Cosine distance between u and v, defined as\n",
    "$1 - \\frac{u \\cdot v}{||u||_2 ||v||_2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the similarity function\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "def similarity(u,v):\n",
    "    return 1 - spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocssing the query vector\n",
    "q = 'bmw dortmund zeiss zeiss'\n",
    "query = []\n",
    "q_v = np.zeros(len(word2index))\n",
    "for w in q.split():\n",
    "    q_v[word2index[w]]+=1\n",
    "    query.append(word2index[w])\n",
    "sum_ = np.sum(q_v)\n",
    "def normalize_query(i,sum_):\n",
    "    return 0.5+(0.5*i)/sum_\n",
    "norm_q = np.vectorize(normalize_query)\n",
    "q_v = norm_q(q_v,sum_)\n",
    "\n",
    "# normalizing the query\n",
    "q_v = np.multiply(q_v,inv_doc_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most relevant documents according to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k(k,doc_sim,index2document):\n",
    "    for doc,sim in sorted(doc_sim.items(),key = lambda x:x[1], reverse=True):\n",
    "        print (index2document[doc])\n",
    "        k-=1\n",
    "        if k==0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### q is mapped to the k-concept space\n",
    "$q=q^{T}U_k\\sum^{-1}_{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent semantic indexing (LSI)\n",
    "from numpy import linalg\n",
    "#query:indices of words appear in matrix\n",
    "#k:dimention of latent space\n",
    "def latent_semantic_indexing(query, tf_idf, k, topk, index2document):\n",
    "    doc_similarity = {}\n",
    "    u,s,v = linalg.svd(tf_idf)\n",
    "    u = u[:,:k]\n",
    "    s_ = np.zeros((k,k))\n",
    "    for i in range(k):\n",
    "        s_[i,i] = s[i]\n",
    "    v = v[:k,:]\n",
    "    q_v = np.zeros(len(word2index))\n",
    "    for q in query:\n",
    "        q_v[q]+=1\n",
    "    q_v = q_v.reshape(1,-1)\n",
    "    q_v = np.matmul(q_v,u)\n",
    "    s_ = linalg.inv(s_)\n",
    "    q_v = np.matmul(q_v,s_)\n",
    "    \n",
    "    for i in range(v.shape[1]):\n",
    "        sim = similarity(q_v,v[:,i])\n",
    "        doc_similarity[i] = sim \n",
    "    find_top_k(topk, doc_similarity,index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarlZeissMeditec-AnnualReport-2017.json\n",
      "CarlZeissMeditec-AnnualReport-2015.json\n",
      "CarlZeissMeditec-AnnualReport-2016.json\n",
      "BMW-AnnualReport-2015.json\n",
      "BMW-AnnualReport-2017.json\n"
     ]
    }
   ],
   "source": [
    "latent_semantic_indexing(query, tf_idf, 3, 5, index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
