{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**80:20 rule of data science**\n",
    "\n",
    "- 80% of the work are spent on pre-processing, data cleansing\n",
    "- 20% of the work are spent on data analysis and visualization\n",
    "\n",
    "**VERY IMPORTANT ON PRE-PROCESSING**\n",
    "\n",
    "In the following, we try preprocessing using NLTK and Spacy and further discuss the good and bad of both libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import string\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using nltk packages \n",
    "[NLTK](https://www.nltk.org) is a leading platform for building Python programs to work with human language data.\n",
    "\n",
    "First we try to use nltk packages to pre-process the data based on:\n",
    "- strong community support\n",
    "- more efficient than spacy (less run time after comparison)\n",
    "\n",
    "However, there are some drawbacks, especially for German processing:\n",
    "- lemmatization not supported\n",
    "- bad stemming --> casuing too many redundant tokens\n",
    "- part-of-speech tagging not well integrated\n",
    "\n",
    "Our NLTK preprocessing consists of the following steps:\n",
    "1. remove digits (e.g. '0123456789')\n",
    "2. remove punctuations (e.g. ',.„“|')\n",
    "3. change all text case to lower case\n",
    "4. tokenize sentences (breaking sentences into words or phrases)\n",
    "5. remove stop words (e.g. 'einer', 'eine', 'eines', 'einen', 'oder', 'aber'...)\n",
    "6. (stemming words (e.g. 'sucht | suchst' -> 'suchen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def nltkPreprocessing(text):\n",
    "    \n",
    "    #remove digits and some special symbols\n",
    "    dig_translator = str.maketrans('', '', '0123456789-/€®–„“|')\n",
    "    text = text.translate(dig_translator)\n",
    "    \n",
    "    #remove punctuation\n",
    "    str_translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(str_translator).lower()\n",
    "    text = text.strip()\n",
    "    \n",
    "    #tokenize sentences\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words('german')\n",
    "    \n",
    "    #remove stop words\n",
    "    filtered_tokens = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "    #using PorterStemmer to stem the tokens (effect not good)\n",
    "#     ps = SnowballStemmer('german')\n",
    "#     stem_tokens = [ps.stem(w) for w in filtered_tokens]   \n",
    "#     return stem_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using Spacy packages\n",
    "[Spacy](https://spacy.io/usage/) is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more. \n",
    "\n",
    "Based on the features, Spacy gives a better performance in:\n",
    "- tokenization quality\n",
    "- lemmatization\n",
    "\n",
    "However, there are some drawbacks, especially for German processing:\n",
    "- super slow tokenization\n",
    "- incomplete german stop words \n",
    "\n",
    "Our Spacy preprocessing consists of the following steps:\n",
    "1. change all text case to lower case\n",
    "2. tokenize sentences (breaking sentences into words or phrases)\n",
    "3. only keep words with alphabets \n",
    "4. remove words with less than 3 letters \n",
    "5. remove stop words (e.g. 'einer', 'eine', 'eines', 'einen', 'oder', 'aber'...)\n",
    "6. remove punctuations (e.g. ',.„“|')\n",
    "7. remove currency signs\n",
    "8. remove number-like words (e.g. 'one', 'two'..)\n",
    "9. remove spaces\n",
    "10. lemmatize words\n",
    "\n",
    "After comparison with nltk, we indeed find out that spacy provides more accurate text preprocessing on German text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load German language package in spacy\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "nlp = spacy.load('de', disable=['parser', 'ner'])\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "def spacyPreprocessing(text): \n",
    "    # define stop words\n",
    "    my_stop_words  = ['einer', 'eine', 'eines', 'einen', 'oder', 'aber', 'dass',  'teur', 'euro', 'eur', 'jahr', 'million', 'tausend', 'mio', 'mrd']\n",
    "    stop_words = stopwords.words('german')\n",
    "    stop_words.extend(my_stop_words)\n",
    "    for w in stop_words:\n",
    "        nlp.vocab[w].is_stop = True\n",
    "    \n",
    "    #tokenize texts\n",
    "    word_tokens = nlp(text.lower())\n",
    "    \n",
    "    #remove words containing special letters, short words, stop words, punctuations, currency, numbers and spaces, then lemmatize words\n",
    "    final_word_tokens = [w.lemma_ for w in word_tokens if w.text.isalpha() and len(w)>2 and not w.is_stop and not w.is_punct and not w.is_currency and not w.like_num and not w.is_space]\n",
    "    \n",
    "    return final_word_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
