{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Generation\n",
    "\n",
    "After preprocessing, we want to generate text corpus for each document, which consists of a list of duplicated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run 02-Preprocessing.ipynb first, to make sure preprocessing function can be invoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 02-Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate Coupus for each document\n",
    "\n",
    "- target_corpus: folder to save the corpus for each document (/spacy_corpus folder)\n",
    "- source_folder: folder contain Pre-extracted TXT files (/txt_data folder)\n",
    "- spacyFlag: default: True, if set as True, to use spacy preprocessing, otherwise using nltk preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateCorpus(target_corpus, source_folder, spacyFlag=True):\n",
    "    if not os.path.exists(target_corpus):\n",
    "        os.makedirs(target_corpus)\n",
    "    \n",
    "    #monitor the file process\n",
    "    progress = 0\n",
    "    progress_count = 0\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        file_count = len(files)\n",
    "        for f in files:\n",
    "            progress += 1\n",
    "            progress_count += 1\n",
    "            tokens = []\n",
    "            \n",
    "            try:\n",
    "                with open(root+'/'+f) as fr:\n",
    "                    text = fr.read()\n",
    "                    if spacyFlag == False:\n",
    "                        tokens = nltkPreprocessingx(text)\n",
    "                    if spacyFlag == True:\n",
    "                        tokens = spacyPreprocessing(text)\n",
    "            except:\n",
    "                print('Error while processing file: ', f)\n",
    "            \n",
    "            \n",
    "            with open(target_corpus+f, 'wb') as fw:\n",
    "                    #use pickle library to dump list to file\n",
    "                    pickle.dump(tokens, fw)\n",
    "\n",
    "            if progress_count % 400 == 0:\n",
    "                print('{0}% of total files has been processed'.format(round(progress/file_count*100), 2))\n",
    "                print('running time: {0}'.format(time.process_time() - start_time))\n",
    "\n",
    "            if progress_count == file_count:\n",
    "                print('all files has been processed')\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the snippets below, make sure those txt files are prepared. If not, go to [Convert JSON to TXT](01-Convert_JSON_to_TXT.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9% of total files has been processed\n",
      "running time: 2118.1351720000002\n",
      "18% of total files has been processed\n",
      "running time: 4186.865105999999\n",
      "27% of total files has been processed\n",
      "running time: 6081.383608\n",
      "36% of total files has been processed\n",
      "running time: 7842.223702000001\n",
      "45% of total files has been processed\n",
      "running time: 9657.111828\n",
      "54% of total files has been processed\n",
      "running time: 11528.062233999999\n",
      "63% of total files has been processed\n",
      "running time: 13594.633854000002\n",
      "72% of total files has been processed\n",
      "running time: 15608.362729999999\n",
      "81% of total files has been processed\n",
      "running time: 17494.233487999998\n",
      "90% of total files has been processed\n",
      "running time: 19390.351499999997\n",
      "99% of total files has been processed\n",
      "running time: 21255.462284\n",
      "all files has been processed\n",
      "CPU times: user 2h 58min 44s, sys: 19min 22s, total: 3h 18min 7s\n",
      "Wall time: 7h 44min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#please modify the path\n",
    "# target_corpus = '/home/bit/ma0/LabShare/data/chui_ma/spacy_corpus/'\n",
    "# source_folder = '/home/bit/ma0/LabShare/data/chui_ma/txt_data/'\n",
    "\n",
    "#relative folder path\n",
    "target_corpus = '../spacy_corpus/'\n",
    "source_folder = '../txt_data/'\n",
    "\n",
    "generateCorpus(target_corpus, source_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
