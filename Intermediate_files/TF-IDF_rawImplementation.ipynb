{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run preprocessing\n",
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF using self-implemented method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obtain term-frequency matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def tf_idf_processing(raw_doc_path):\n",
    "    word2index = {}\n",
    "    document2index = {}\n",
    "    index2document = {}\n",
    "    document_word_vectors = {}\n",
    "    w_cnt = 0\n",
    "    d_cnt = 0\n",
    "    for root, dirs, files in os.walk(raw_doc_path):\n",
    "        for f in files:\n",
    "            print('.', end='')\n",
    "            document_word_vectors[f] = []\n",
    "            document2index[f] = d_cnt\n",
    "            index2document[d_cnt] = f\n",
    "            d_cnt+=1\n",
    "            with open(root+'/'+f) as fs:\n",
    "                try:\n",
    "                    for line in fs:\n",
    "                        #loads json file, preprocess the content\n",
    "                        obj = json.loads(line)\n",
    "                        textType = obj['type']\n",
    "                        if textType == 'paragraph':\n",
    "                            words = preprocessing_spacy(obj['content'].lower())\n",
    "                            for w in words:\n",
    "                                if w not in word2index:\n",
    "                                    #reserve word_index dict, for referening later on\n",
    "                                    word2index[w] = w_cnt\n",
    "                                    w_cnt+=1\n",
    "                                document_word_vectors[f].append(word2index[w])\n",
    "                except:\n",
    "                    print ('Error while processing: ',f)\n",
    "\n",
    "    #create word_frequency matrix                        \n",
    "    w_f_matrix = np.zeros((len(word2index),len(document2index)))\n",
    "    for doc in document_word_vectors:\n",
    "        i = document2index[doc]\n",
    "        for j in document_word_vectors[doc]:\n",
    "            w_f_matrix[j,i]+=1 \n",
    "\n",
    "    # obtain normalized term-frequency matrix\n",
    "    t_f = np.copy(w_f_matrix)\n",
    "    sum_f = np.zeros(len(document2index))\n",
    "    for i in range(len(document2index)):\n",
    "        sum_f[i] = np.sum(t_f[:,i])\n",
    "    t_f = np.divide(t_f,sum_f)  \n",
    "\n",
    "    # obtaining tf-idf matrix\n",
    "    inv_doc_freq = np.count_nonzero(t_f,axis=1)\n",
    "    def normalize(a,x):\n",
    "        return np.log(x/a)\n",
    "    norm = np.vectorize(normalize)\n",
    "    inv_doc_freq = norm(inv_doc_freq,len(document2index))\n",
    "    raw_tf_idf = np.multiply(t_f,inv_doc_freq.reshape(-1,1))\n",
    "    return raw_tf_idf, word2index, index2document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_words(tf_idf, word2index, index2document, k):\n",
    "    document_freq_words = {}\n",
    "    for i in range(tf_idf.shape[1]):\n",
    "        freq_words = []\n",
    "        #get indices of k-maximum values in numpy column\n",
    "        index = np.argpartition(tf_idf[:, i], -k)[-k:]\n",
    "        index = index[np.argsort(tf_idf[:, i][index])]\n",
    "        for ind in index:\n",
    "            #find frequent words with coresponding index\n",
    "            freq_words.append(list(word2index.keys())[list(word2index.values()).index(ind)])\n",
    "        filename = index2document[i].split('.')[0]\n",
    "        document_freq_words[filename] = freq_words\n",
    "        df = pd.DataFrame(document_freq_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# raw_doc_path = '/home/bit/ma0/LabShare/data/three-companies/json'\n",
    "# raw_tf_idf, word2index, index2document = tf_idf_processing(raw_doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most frequent keywords using self implemented tf-idf library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_top_k_words(raw_tf_idf, word2index, index2document, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF on preprocessed documents, to find most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def tf_idf_PrerocessedDoc(processed_docu_path, document_limit):\n",
    "    word2index = {}\n",
    "    document2index = {}\n",
    "    index2document = {}\n",
    "    document_word_vectors = {}\n",
    "    w_cnt = 0\n",
    "    d_cnt = 0\n",
    "    for root, dirs, files in os.walk(processed_docu_path):\n",
    "        for f in files:\n",
    "            print('.', end='')\n",
    "            document_limit -= 1\n",
    "            document_word_vectors[f] = []\n",
    "            document2index[f] = d_cnt\n",
    "            index2document[d_cnt] = f\n",
    "            d_cnt+=1\n",
    "            with open(root+'/'+f, 'rb') as fs:\n",
    "                try:\n",
    "                    words = pickle.load(fs)\n",
    "                    for w in words:\n",
    "                        if w not in word2index:\n",
    "                            word2index[w] = w_cnt\n",
    "                            w_cnt += 1\n",
    "                        document_word_vectors[f].append(word2index[w])\n",
    "                except:\n",
    "                    print ('Error while processing: ',f)\n",
    "            if document_limit == 0:\n",
    "                break\n",
    "\n",
    "    #create word_frequency matrix                        \n",
    "    w_f_matrix = np.zeros((len(word2index),len(document2index)))\n",
    "    for doc in document_word_vectors:\n",
    "        i = document2index[doc]\n",
    "        for j in document_word_vectors[doc]:\n",
    "            w_f_matrix[j,i]+=1 \n",
    "\n",
    "    # obtain normalized term-frequency matrix\n",
    "    t_f = np.copy(w_f_matrix)\n",
    "    sum_f = np.zeros(len(document2index))\n",
    "    for i in range(len(document2index)):\n",
    "        sum_f[i] = np.sum(t_f[:,i])\n",
    "    t_f = np.divide(t_f,sum_f)  \n",
    "\n",
    "    # obtaining tf-idf matrix\n",
    "    inv_doc_freq = np.count_nonzero(t_f,axis=1)\n",
    "    def normalize(a,x):\n",
    "        return np.log(x/a)\n",
    "    norm = np.vectorize(normalize)\n",
    "    inv_doc_freq = norm(inv_doc_freq,len(document2index))\n",
    "    new_tf_idf = np.multiply(t_f,inv_doc_freq.reshape(-1,1))\n",
    "    return new_tf_idf, word2index, index2document, inv_doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# processed_doc_path = './LabShare/data/chui_ma/spacy_corpus'\n",
    "# new_tf_idf, new_word2index, new_index2document, inv_doc_freq = tf_idf_PrerocessedDoc(processed_doc_path, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# find_top_k_words(new_tf_idf, new_word2index, new_index2document, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
