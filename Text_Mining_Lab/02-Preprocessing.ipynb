{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**80:20 rule of data science** (Quote from \"Machine Learning for Text Analysis\" slide 48,  provided by Prof. Christian Bauckhage)\n",
    "\n",
    "- 80% of the work are spent on pre-processing, data cleansing\n",
    "- 20% of the work are spent on data analysis and visualization\n",
    "\n",
    "**VERY IMPORTANT ON PRE-PROCESSING**\n",
    "\n",
    "In the following, we try preprocessing using NLTK and Spacy and further discuss the benefits and drawbacks of both libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using nltk packages \n",
    "[NLTK](https://www.nltk.org) is a leading platform for building Python programs to work with human language data [1].\n",
    "\n",
    "First we try to use nltk packages to pre-process the data based on:\n",
    "- strong community support\n",
    "- more efficient than spacy (less run time after comparison)\n",
    "\n",
    "However, there are some drawbacks, especially for German processing:\n",
    "- lemmatization not supported\n",
    "- bad stemming --> casuing too many redundant tokens\n",
    "- part-of-speech tagging not well integrated\n",
    "\n",
    "Our NLTK preprocessing consists of the following steps:\n",
    "1. remove digits (e.g. '0123456789')\n",
    "2. remove punctuations (e.g. ',.„“|')\n",
    "3. change all text case to lower case\n",
    "4. tokenize sentences (breaking sentences into words or phrases)\n",
    "5. remove stop words (e.g. 'einer', 'eine', 'eines', 'einen', 'oder', 'aber'...)\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "[1] Bird, Steven, Edward Loper and Ewan Klein (2009). Natural Language Processing with Python.  O'Reilly Media Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltkPreprocessing(text):\n",
    "\n",
    "    # remove digits and some special symbols\n",
    "    dig_translator = str.maketrans('', '', '0123456789-/€®–„“|')\n",
    "    text = text.translate(dig_translator)\n",
    "\n",
    "    # remove punctuation\n",
    "    str_translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(str_translator).lower()\n",
    "    text = text.strip()\n",
    "\n",
    "    # tokenize sentences\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words('german')\n",
    "\n",
    "    # remove stop words\n",
    "    filtered_tokens = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "    # using PorterStemmer to stem the tokens (effect not good)\n",
    "#     ps = SnowballStemmer('german')\n",
    "#     stem_tokens = [ps.stem(w) for w in filtered_tokens]\n",
    "#     return stem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using Spacy packages\n",
    "[Spacy](https://spacy.io/usage/) is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more. \n",
    "\n",
    "Based on the features, Spacy gives a better performance in:\n",
    "- tokenization quality\n",
    "- lemmatization\n",
    "\n",
    "However, there are some drawbacks, especially for German processing:\n",
    "- super slow tokenization\n",
    "- incomplete german stop words \n",
    "\n",
    "Our Spacy preprocessing consists of the following steps:\n",
    "1. change all text case to lower case\n",
    "2. tokenize sentences (breaking sentences into words or phrases)\n",
    "3. only keep words with alphabets \n",
    "4. remove words with less than 3 letters \n",
    "5. remove stop words (e.g. 'einer', 'eine', 'eines', 'einen', 'oder', 'aber'...)\n",
    "6. remove punctuations (e.g. ',.„“|')\n",
    "7. remove currency signs\n",
    "8. remove number-like words (e.g. 'one', 'two'..)\n",
    "9. remove spaces\n",
    "10. lemmatize words\n",
    "\n",
    "After comparison with nltk, we indeed find out that spacy provides more accurate text preprocessing on German text.\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "[1] Honnibal, Matthew and Montani, Ines (2017). \"Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load German language package in spacy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load('de', disable=['parser', 'ner'])\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "\n",
    "def spacyPreprocessing(text):\n",
    "    # define stop words\n",
    "    my_stop_words = ['einer', 'eine', 'eines', 'einen', 'oder', 'aber', 'dass',\n",
    "                     'teur', 'euro', 'eur', 'jahr', 'million', 'tausend', 'mio', 'mrd']\n",
    "    stop_words = stopwords.words('german')\n",
    "    stop_words.extend(my_stop_words)\n",
    "    for w in stop_words:\n",
    "        nlp.vocab[w].is_stop = True\n",
    "\n",
    "    # tokenize texts\n",
    "    word_tokens = nlp(text.lower())\n",
    "\n",
    "    # remove words containing special letters, short words, stop words, punctuations,\n",
    "    # currency, numbers and spaces, then lemmatize words\n",
    "    final_word_tokens = [w.lemma_ for w in word_tokens if w.text.isalpha() and len(w)>2 \n",
    "                         and not (w.is_stop or w.is_punct or w.is_currency or w.like_num or w.is_space)]\n",
    "\n",
    "    return final_word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between NLTK Preprocessing and Spacy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.4 ms, sys: 4.75 ms, total: 82.1 ms\n",
      "Wall time: 82.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# randomly choose one file for comparison\n",
    "test_file = '../txt_data/Adidas-QuarterlyReport-2016-Q3'\n",
    "with open(test_file) as f:\n",
    "    text = f.read()\n",
    "    nltk_tokens = nltkPreprocessing(text)\n",
    "nltk_count = Counter(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 88.7 ms, total: 1.29 s\n",
      "Wall time: 600 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(test_file) as f:\n",
    "    text = f.read()\n",
    "    spacy_tokens = spacyPreprocessing(text)\n",
    "spacy_count = Counter(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to compare these two preprocessing methods based on the following criteria:\n",
    "1. Run Time\n",
    "2. Number of total tokens after preprocessing\n",
    "3. Number of unique tokens after preprocessing\n",
    "4. occurrence of some words (e.g. geben), indicating the effectiveness of lemmatization (gab|gibt -> geben)\n",
    "5. **Most common tokens** comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>NLTK</th>\n",
    "    <th>Spacy</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Run Time (ms)</td>\n",
    "    <td>~82</td>\n",
    "    <td>~600</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td># of total tokens</td>\n",
    "    <td>4619</td>\n",
    "    <td>3804</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td># of unique tokens</td>\n",
    "    <td>1636</td>\n",
    "    <td>1289</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td># of \"geben\"</td>\n",
    "    <td>1</td>\n",
    "    <td>3</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td># of \"gab\"</td>\n",
    "    <td>2</td>\n",
    "    <td>0</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLTK_Tokens</th>\n",
       "      <th>NLTK_Frequency</th>\n",
       "      <th>Spacy_Tokens</th>\n",
       "      <th>Spacy_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adidas</td>\n",
       "      <td>82</td>\n",
       "      <td>adidas</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umsatz</td>\n",
       "      <td>76</td>\n",
       "      <td>umsatz</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sowie</td>\n",
       "      <td>64</td>\n",
       "      <td>aufwendungen</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mio</td>\n",
       "      <td>55</td>\n",
       "      <td>konzerns</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aufwendungen</td>\n",
       "      <td>41</td>\n",
       "      <td>betrieblich</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>konzerns</td>\n",
       "      <td>37</td>\n",
       "      <td>hoch</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>aufgrund</td>\n",
       "      <td>35</td>\n",
       "      <td>steigen</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mrd</td>\n",
       "      <td>35</td>\n",
       "      <td>entwicklung</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>betrieblichen</td>\n",
       "      <td>34</td>\n",
       "      <td>prozentpunkte</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prozentpunkte</td>\n",
       "      <td>33</td>\n",
       "      <td>erhöhen</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NLTK_Tokens  NLTK_Frequency   Spacy_Tokens  Spacy_Frequency\n",
       "0         adidas              82         adidas               82\n",
       "1         umsatz              76         umsatz               74\n",
       "2          sowie              64   aufwendungen               41\n",
       "3            mio              55       konzerns               37\n",
       "4   aufwendungen              41    betrieblich               37\n",
       "5       konzerns              37           hoch               34\n",
       "6       aufgrund              35        steigen               33\n",
       "7            mrd              35    entwicklung               33\n",
       "8  betrieblichen              34  prozentpunkte               33\n",
       "9  prozentpunkte              33        erhöhen               30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe consisting of nltk word tokens and word frequency\n",
    "nltk_df = pd.DataFrame(nltk_count, index=['NLTK_Frequency'])\n",
    "nltk_df = nltk_df.transpose().sort_values('NLTK_Frequency', ascending=False)[:10]\n",
    "nltk_df['NLTK_Tokens'] = nltk_df.index\n",
    "nltk_df = nltk_df[['NLTK_Tokens', 'NLTK_Frequency']]\n",
    "nltk_df.index = range(len(nltk_df))\n",
    "\n",
    "# dataframe consisting of spacy word tokens and word frequency\n",
    "spacy_df = pd.DataFrame(spacy_count, index=['Spacy_Frequency'])\n",
    "spacy_df = spacy_df.transpose().sort_values('Spacy_Frequency', ascending=False)[:10]\n",
    "spacy_df['Spacy_Tokens'] = spacy_df.index\n",
    "spacy_df = spacy_df[['Spacy_Tokens', 'Spacy_Frequency']]\n",
    "spacy_df.index = range(len(spacy_df))\n",
    "\n",
    "pd.concat([nltk_df, spacy_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just take one file as an example, regarding the running time, nltk preprocessing is more efficient. However, spacy preprocessing generate more reliable results, especially on token lemmatization. \n",
    "\n",
    "After file preprocessing, we would like to generate corpus [Corpus Generation](03-Corpus_Generation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
