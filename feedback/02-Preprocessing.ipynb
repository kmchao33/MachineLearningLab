{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**80:20 rule of data science**\n",
    "\n",
    "- 80% of the work are spent on pre-processing, data cleansing\n",
    "- 20% of the work are spent on data analysis and visualization\n",
    "\n",
    "**VERY IMPORTANT ON PRE-PROCESSING**\n",
    "\n",
    "In the following, we try preprocessing using NLTK and Spacy and further discuss the good and bad of both libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"/>\n",
    "\n",
    "### dsp:\n",
    "  * Is this rule true? Is this a quote?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import string\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"/>\n",
    "\n",
    "### dsp:\n",
    "  * I'd prefer to list the general packages first and add some structure by inserting empty lines.\n",
    "  * Are you sure that all warning that you silence will be unimportant? You might want to add a comment describing, which comment you want to silence and for what reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using nltk packages \n",
    "[NLTK](https://www.nltk.org) is a leading platform for building Python programs to work with human language data.\n",
    "\n",
    "First we try to use nltk packages to pre-process the data based on:\n",
    "- strong community support\n",
    "- more efficient than spacy (less run time after comparison)\n",
    "\n",
    "However, there are some drawbacks, especially for German processing:\n",
    "- lemmatization not supported\n",
    "- bad stemming --> casuing too many redundant tokens\n",
    "- part-of-speech tagging not well integrated\n",
    "\n",
    "Our NLTK preprocessing consists of the following steps:\n",
    "1. remove digits (e.g. '0123456789')\n",
    "2. remove punctuations (e.g. ',.„“|')\n",
    "3. change all text case to lower case\n",
    "4. tokenize sentences (breaking sentences into words or phrases)\n",
    "5. remove stop words (e.g. 'einer', 'eine', 'eines', 'einen', 'oder', 'aber'...)\n",
    "6. (stemming words (e.g. 'sucht | suchst' -> 'suchen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"/>\n",
    "\n",
    "### dsp:\n",
    "  * &#x1f642; It is great that you make an attempt to compare the two toolkits and give your criteria.\n",
    "  * You could support your claim with a few short experiments. Take one or two sentences and process them with the two toolkits and compare the results. Although these special examples do not \"prove\" anything about how the toolkits compare in general, it helps to understand the differences. (If you wanted to provide a strong comparison, you would need to find or create a comprehensive benchmark. A bit too much for a student project.)\n",
    "  * How do you know that \"NLTK is a leading platform for building Python programs to work with human language data.\"? &#x1f609; Is it true?\n",
    "  * Does the stemmer map  'sucht ' and 'suchst' to 'suchen'? (I did not check.)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def nltkPreprocessing(text):\n",
    "    \n",
    "    #remove digits and some special symbols\n",
    "    dig_translator = str.maketrans('', '', '0123456789-/€®–„“|')\n",
    "    text = text.translate(dig_translator)\n",
    "    \n",
    "    #remove punctuation\n",
    "    str_translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(str_translator).lower()\n",
    "    text = text.strip()\n",
    "    \n",
    "    #tokenize sentences\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words('german')\n",
    "    \n",
    "    #remove stop words\n",
    "    filtered_tokens = [w.lower() for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "    #using PorterStemmer to stem the tokens (effect not good)\n",
    "#     ps = SnowballStemmer('german')\n",
    "#     stem_tokens = [ps.stem(w) for w in filtered_tokens]   \n",
    "#     return stem_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing using Spacy packages\n",
    "[Spacy](https://spacy.io/usage/) is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more. \n",
    "\n",
    "Based on the features, Spacy gives a better performance in:\n",
    "- tokenization quality\n",
    "- lemmatization\n",
    "\n",
    "However, there are some drawbacks, especially for German processing:\n",
    "- super slow tokenization\n",
    "- incomplete german stop words \n",
    "\n",
    "Our Spacy preprocessing consists of the following steps:\n",
    "1. change all text case to lower case\n",
    "2. tokenize sentences (breaking sentences into words or phrases)\n",
    "3. only keep words with alphabets \n",
    "4. remove words with less than 3 letters \n",
    "5. remove stop words (e.g. 'einer', 'eine', 'eines', 'einen', 'oder', 'aber'...)\n",
    "6. remove punctuations (e.g. ',.„“|')\n",
    "7. remove currency signs\n",
    "8. remove number-like words (e.g. 'one', 'two'..)\n",
    "9. remove spaces\n",
    "10. lemmatize words\n",
    "\n",
    "After comparison with nltk, we indeed find out that spacy provides more accurate text preprocessing on German text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load German language package in spacy\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "nlp = spacy.load('de', disable=['parser', 'ner'])\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "def spacyPreprocessing(text): \n",
    "    # define stop words\n",
    "    my_stop_words  = ['einer', 'eine', 'eines', 'einen', 'oder', 'aber', 'dass',  'teur', 'euro', 'eur', 'jahr', 'million', 'tausend', 'mio', 'mrd']\n",
    "    stop_words = stopwords.words('german')\n",
    "    stop_words.extend(my_stop_words)\n",
    "    for w in stop_words:\n",
    "        nlp.vocab[w].is_stop = True\n",
    "    \n",
    "    #tokenize texts\n",
    "    word_tokens = nlp(text.lower())\n",
    "    \n",
    "    #remove words containing special letters, short words, stop words, punctuations, currency, numbers and spaces, then lemmatize words\n",
    "    final_word_tokens = [w.lemma_ for w in word_tokens if w.text.isalpha() and len(w)>2 and not w.is_stop and not w.is_punct and not w.is_currency and not w.like_num and not w.is_space]\n",
    "    \n",
    "    return final_word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"/>\n",
    "\n",
    "### dsp:\n",
    "  * I didn't know that you can convince spacy to process even larger texts. Nice! &#x1f642;\n",
    "  * I neither did know that you can tell spacy to consider certain words to be stop words. &#x1f642;\n",
    "  * I would not be suprised if the results for `nlp(text)` would be better than for `nlp(text.lower())`, but I have not tried it yet.\n",
    "  * Please distribute the list comprehension over more than one line so that we can read it without scrolling. Linebreaks are fine:\n",
    "```Python\n",
    "    final_word_tokens = [w.lemma_ for w in word_tokens if w.text.isalpha() and len(w)>2 \n",
    "                             and not w.is_stop and not w.is_punct and not w.is_currency \n",
    "                             and not w.like_num and not w.is_space]\n",
    "```\n",
    "or\n",
    "```Python\n",
    "    final_word_tokens = [w.lemma_ for w in word_tokens if w.text.isalpha() and len(w)>2 \n",
    "                             and not (w.is_stop or w.is_punct or w.is_currency or w.like_num or w.is_space)]\n",
    "```\n",
    "  * Did you check whether `my_stop_words` is not subsumed by `stopwords.words('german')`? The list looks similar to my early list into which I had not invest much thought.\n",
    "  * It would be nice to demonstrate these two functions in this notebook with small examples. Maybe even to the degree to convince the reader of your comparison results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
