{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import pyLDAvis.gensim\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "def initialLDAModel(folder_path, non_german_file_path, topic_num=50, document_limit=1000):\n",
    "    #get a list of non_german files\n",
    "    non_german_files = []\n",
    "    with open(non_german_file_path, 'r') as fr:\n",
    "        for line in fr:\n",
    "            non_german_files.append(line.strip())\n",
    "    \n",
    "    dictionary = []\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            if f[:-11] not in non_german_files:\n",
    "                document_limit -= 1\n",
    "                try:\n",
    "                    with open(root+'/'+f, 'rb') as fr:\n",
    "                        filenames.append(f)\n",
    "                        document_tokens = pickle.load(fr)\n",
    "                        dictionary.append(document_tokens)\n",
    "                except:\n",
    "                    print('Error while processing: ', f)\n",
    "            \n",
    "            if document_limit == 0:\n",
    "                break\n",
    "    gensim_dictionary = Dictionary(dictionary)\n",
    "    corpus = [gensim_dictionary.doc2bow(text) for text in dictionary]\n",
    "    lda = LdaModel(corpus, num_topics=topic_num, id2word=gensim_dictionary, iterations=200)\n",
    "    topics = lda.show_topics(num_topics=-1, num_words=20)\n",
    "    \n",
    "    doc_pos = 0\n",
    "    mat = np.zeros((len(filenames), topic_num))\n",
    "\n",
    "    for doc in corpus:\n",
    "        vector = lda[doc] # get topic probability distribution for a document\n",
    "        for element in vector:\n",
    "            mat[doc_pos][element[0]] = element[1]\n",
    "        doc_pos += 1\n",
    "        \n",
    "    df = pd.DataFrame(mat, index=filenames, columns=range(0,topic_num))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# warnings.filterwarnings(\"ignore\") \n",
    "# non_german_file_path = './LabShare/data/non_german_files.txt'\n",
    "# processed_doc_path = './LabShare/data/chui_ma/spacy_corpus'\n",
    "# df = initialLDAModel(processed_doc_path, non_german_file_path, 100, 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 3.2 s, total: 1min 26s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "non_german_file_path = './LabShare/data/non_german_files.txt'\n",
    "data_path = '/home/bit/ma0/LabShare/data/chui_ma/presentation_LDA'\n",
    "df = initialLDAModel(data_path, non_german_file_path, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Topic_Related_Doc(df):    \n",
    "    topic_related_documents = []\n",
    "    for column in range(df.shape[1]):\n",
    "        row_count = 0\n",
    "        for row in range(df.shape[0]):\n",
    "            if df.iat[row, column] > 0.3:\n",
    "                row_count += 1\n",
    "        topic_related_documents.append(df.nlargest(row_count, column).index)\n",
    "    return topic_related_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierachicalLDA(folder_path, non_german_file_path, topic_related_documents, topic_num=5, document_limit=1000):\n",
    "    #get a list of non_german files\n",
    "    non_german_files = []\n",
    "    with open(non_german_file_path, 'r') as fr:\n",
    "        for line in fr:\n",
    "            non_german_files.append(line.strip())\n",
    "            \n",
    "    dictionary = []\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for f in sorted(files):\n",
    "            if f in topic_related_documents:\n",
    "                if f[:-11] not in non_german_files:\n",
    "                    with open(root+'/'+f, 'rb') as fr:\n",
    "                        filenames.append(f)\n",
    "                        document_tokens = pickle.load(fr)\n",
    "                        dictionary.append(document_tokens)\n",
    "\n",
    "    gensim_dictionary = Dictionary(dictionary)\n",
    "    corpus = [gensim_dictionary.doc2bow(text) for text in dictionary]\n",
    "    lda = models.ldamodel.LdaModel(corpus, num_topics=topic_num, id2word=gensim_dictionary, iterations=200)\n",
    "    topics = lda.show_topics(num_topics=-1, num_words=20)\n",
    "    return lda, corpus, gensim_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_Topic_Related_Doc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(data_path, docs, topic_num):\n",
    "    if len(docs) > 5:\n",
    "        lda, corpus, dictionary = hierachicalLDA(data_path, non_german_file_path, docs, topic_num)\n",
    "        return pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "    else:\n",
    "        print('Cannot visualize, Too less documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# visualization(docs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization(data_path, docs[0].tolist(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
