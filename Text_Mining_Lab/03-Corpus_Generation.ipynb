{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Generation\n",
    "\n",
    "After preprocessing, we want to generate text corpus for each document, which consists of a list of potentially duplicated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run 02-Preprocessing.ipynb first, to make sure preprocessing function can be invoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 299 ms, sys: 3.53 ms, total: 303 ms\n",
      "Wall time: 77.1 ms\n",
      "CPU times: user 1.39 s, sys: 76.1 ms, total: 1.47 s\n",
      "Wall time: 548 ms\n"
     ]
    }
   ],
   "source": [
    "%run 02-Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generate Coupus for each document\n",
    "\n",
    "- source_folder: folder contain Pre-extracted TXT files (/txt_data folder)\n",
    "- corpus_folder: folder to save the corpus for each document (/spacy_corpus folder)\n",
    "- spacyFlag: default: True, if set as True, to use spacy preprocessing, otherwise using nltk preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCorpus(source_folder, corpus_folder, preprocessor=spacyPreprocessing):\n",
    "    os.makedirs(corpus_folder, exist_ok=True)\n",
    "    # monitor the file process\n",
    "    progress_count = 1\n",
    "\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        file_count = len(files)\n",
    "        for file in files:\n",
    "            progress_count += 1\n",
    "            tokens = []\n",
    "\n",
    "            try:\n",
    "                with open(root+'/'+file) as fr:\n",
    "                    text = fr.read()\n",
    "                    tokens = preprocessor(text)\n",
    "                \n",
    "                with open(corpus_folder+file, 'wb') as fw:\n",
    "                    pickle.dump(tokens, fw)\n",
    "                    \n",
    "            except:\n",
    "                print('Error while processing file: ', file)\n",
    "\n",
    "            if progress_count % 200 == 0:\n",
    "                print('{:6.2%} of the total files have been processed'.format(\n",
    "                    progress_count/file_count), end='\\r')\n",
    "            \n",
    "            if progress_count == file_count:\n",
    "                print('all files have been processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.08% of the total files have been processed\r",
      "CPU times: user 10min 49s, sys: 46.8 s, total: 11min 36s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#please modify the path\n",
    "# corpus_folder = '/home/bit/ma0/LabShare/data/chui_ma/spacy_corpus/'\n",
    "# source_folder = '/home/bit/ma0/LabShare/data/chui_ma/txt_data/'\n",
    "\n",
    "#relative folder path\n",
    "corpus_folder = '../spacy_corpus/'\n",
    "source_folder = '../txt_data/'\n",
    "\n",
    "# use spacyPreprocessing\n",
    "generateCorpus(source_folder, corpus_folder, spacyPreprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating corpus, we would like to do some expriments on [TF-IDF](04-TF-IDF_Raw_Implementation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
